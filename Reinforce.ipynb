{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel REINFORCE\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall the REINFORCE algorithm:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t=0}^T \\gamma^t R_t \\nabla_\\theta \\log \\pi_\\theta(A_t|S_t) \\right],\n",
    "$$\n",
    "\n",
    "that can be approximated with Monte-Carlo methods:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\gamma^t R_t^{(i)} \\nabla_\\theta \\log \\pi_\\theta(A_t^{(i)}|S_t^{(i)}).\n",
    "$$\n",
    "\n",
    "It is obvious that the sum over $t$ can not be computed in parallel, as the current state of the environment depends on the previous one. However, the sum over $i$ can be computed in parallel, as the trajectories are independent. Thus, we need to launch $N$ environments in parallel, collect the trajectories, and then compute the gradients. \n",
    "\n",
    "Unfortunately the parallelization can not be perfromed straightforwardly, as independent environments reach the terminal states at different times. Thus, we need to synchronize the environments at the end of each episode. There are two possible ways to do that: truncation and padding.\n",
    "\n",
    "In case of truncation, we terminates the current episode when the first environment reaches the terminal state. This approach is not preferable as it leads to the loss of information, produced by the environments that have not reached the terminal state yet. Padding, on the other hand, is more preferable, as it allows to collect the full trajectories from all the environments. The only thing we need to do is to mask the gradients of the environments that have reached the terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical rationale\n",
    "\n",
    "Recalling the nabla distributive function property:\n",
    "\n",
    "$$ \\nabla_\\theta \\left( \\alpha f(\\theta) \\right) = \\alpha \\nabla_\\theta f(\\theta), $$\n",
    "\n",
    "therefore \n",
    "\n",
    "$$ \n",
    "\\nabla_\\theta \\left(0 \\cdot f(\\theta) \\right) \\equiv 0 \\cdot \\nabla_\\theta f(\\theta) \\equiv 0, \\\\\n",
    "\\nabla_\\theta \\left(1 \\cdot f(\\theta) \\right) \\equiv 1 \\cdot \\nabla_\\theta f(\\theta) \\equiv \\nabla_\\theta f(\\theta).\n",
    "$$\n",
    "\n",
    "Hence, by introducing the padding function $H\\left(S_t^{(i)}\\right)$ we can nullify the gradients of the environments that have reached the terminal state without stopping other environments. The padding function is defined as follows:\n",
    "\n",
    "$$\n",
    "H\\left(S_t^{(i)}\\right) = \\begin{cases}\n",
    "1, & \\text{if } S_k^{(i)} \\text{ is not terminal } \\forall k \\in \\{0,...,t-1\\} \\\\\n",
    "0, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Thus, the gradient for the $\\theta$ for the given environment becomes zero as soon as the environment reaches the terminal state. The final gradient is computed as follows:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\gamma^t R_t^{(i)} H\\left(S_t^{(i)}\\right) \\nabla_\\theta \\log \\pi_\\theta\\left(A_t^{(i)}|S_t^{(i)}\\right).\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "It is know that the gymnasium environments yield tuple of five elements for each step:\n",
    "\n",
    "```python\n",
    "obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "```\n",
    "\n",
    "For the vectorized enviornment, the first dimension of each element is equal to the number of the environments, which is the $N$ in the formula above. Asssuming that, the definition of padding function becomes evident\n",
    "\n",
    "```python\n",
    "class Padding:\n",
    "\n",
    "    def __init__(self, size: int):\n",
    "        \"\"\"Initialize appding function\"\"\"\n",
    "        self._size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the padding function\"\"\"\n",
    "        self._is_terminated = np.full(size=self._size, False)\n",
    "\n",
    "    def __call__(self, terminated: np.ndarray, truncated: np.ndarray) -> torch.Tensor, bool:\n",
    "        \"\"\"Yields the padding values\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        terminated : np.ndarray\n",
    "            The array of the terminal states\n",
    "        truncated : np.ndarray\n",
    "            The array of the truncated states\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The padding values (1 if value should be used, 0 otherwise)\n",
    "        bool\n",
    "            The total termination flag. If True, then all the states have reached the\n",
    "            terminal states and the training loop should be stopped.\n",
    "        \"\"\"\n",
    "        # As the function has a lag, we need firstly yield the previous values\n",
    "        output = torch.Tensor(self._is_terminated.astype(int))\n",
    "        # Then we update the values\n",
    "        done = np.logical_or(terminated, truncated)\n",
    "        self._is_terminated = np.logical_or(self._is_terminated, done)\n",
    "        # As termination flag checked after the step, we can yield it, using the updated\n",
    "        # values\n",
    "        total_termination = np.all(self._is_terminated)\n",
    "        return output, total_termination \n",
    "\n",
    "```\n",
    "\n",
    "In training loop the function can be used as following:\n",
    "\n",
    "```python\n",
    "\n",
    "padding = Padding(size=N)\n",
    "policy: torch.nn.Module = Policy()\n",
    "\n",
    "...\n",
    "\n",
    "gamma = 1\n",
    "for i in range(iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    padding.reset()\n",
    "    memory.reset()\n",
    "    obs, _ = envs.reset()\n",
    "    losses = torch.zeros(N)\n",
    "    \n",
    "    for t in range(T):\n",
    "\n",
    "        action, log_probs = policy(obs)\n",
    "        obs, rewards, terminated, truncated, _ = env.step(action.detached().numpy())\n",
    "        padding_values, total_termination = padding(terminated, truncated)\n",
    "\n",
    "        memory.append(log_probs, rewards, padding_values)\n",
    "\n",
    "        ...\n",
    "\n",
    "        if total_termination:\n",
    "            break\n",
    "    \n",
    "    loss = memory.loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "For simplicity, the padding module can be integrated into `memory` function to reset it without a separated call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from src.agent import PolicyNetworkContinuous, PolicyNetworkDiscrete, train, validate\n",
    "from src.envs import make_env, make_envs\n",
    "from src.utils import mp4_to_gif\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation for the discrete environment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THREADS = 5\n",
    "ITERATIONS = 10\n",
    "GAMMA = 0.90\n",
    "\n",
    "ENV_NAME = \"highway-fast-v0\"\n",
    "\n",
    "FOLDER = \"./results/reinforce\"\n",
    "\n",
    "CONFIG = {\n",
    "    \"observation\": {\n",
    "        # Use an occupancy grid. The grid size and features can be adjusted.\n",
    "        \"type\": \"OccupancyGrid\",  # or \"Kinematics\" / \"TimeToCollision\"\n",
    "        \"grid_size\": [\n",
    "            [-5, 5],\n",
    "            [-5, 5],\n",
    "        ],  # Two dimensions: x from -5 to 5 and y from -5 to 5\n",
    "        \"grid_step\": [2.0, 2.0],  # Specify step for each dimension\n",
    "        \"features\": [\"presence\", \"vx\"],  # presence and relative speed features\n",
    "    },\n",
    "    \"lanes_count\": 4,\n",
    "    \"vehicles_count\": 50,\n",
    "    \"simulation_frequency\": 25,  # adjust as needed\n",
    "    \"policy_frequency\": 5,\n",
    "    \"duration\": 40,  # initial episode duration in seconds\n",
    "    \"action\": {\"type\": \"DiscreteMetaAction\"},  # use the discrete meta-action space\n",
    "    \"offscreen_rendering\": True,\n",
    "    \"collision_reward\": -1,  # The reward received when colliding with a vehicle.\n",
    "    \"reward_speed_range\": [20, 30],\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\",\n",
    "    \"screen_width\": 600,  # [px]\n",
    "    \"screen_height\": 150,  # [px]\n",
    "    \"centering_position\": [0.3, 0.5],\n",
    "    \"scaling\": 5.5,\n",
    "    \"show_trajectories\": False,\n",
    "    \"render_agent\": True,\n",
    "}\n",
    "\n",
    "MAX_LENGTH = CONFIG[\"duration\"] * CONFIG[\"policy_frequency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = make_envs(ENV_NAME, THREADS, config=CONFIG)\n",
    "input_dim = envs.observation_space.shape[1]\n",
    "output_dim = envs.action_space[0].n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "policy = PolicyNetworkDiscrete(input_dim, HIDDEN_DIM, output_dim)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration  1/10:  44%|████▎     | 87/199 [00:11<00:15,  7.42it/s]\n",
      "Iteration  2/10:  50%|████▉     | 99/199 [00:12<00:12,  7.76it/s, length=65, reward=51.1]\n",
      "Iteration  3/10:  74%|███████▍  | 147/199 [00:19<00:06,  7.62it/s, length=53, reward=44.3]\n",
      "Iteration  4/10:  44%|████▎     | 87/199 [00:12<00:15,  7.07it/s, length=97, reward=75.7]\n",
      "Iteration  5/10:  14%|█▎        | 27/199 [00:04<00:27,  6.16it/s, length=44, reward=35.6]"
     ]
    }
   ],
   "source": [
    "model, results = train(policy, envs, optimizer, GAMMA, ITERATIONS, device, SEED)\n",
    "torch.save(model.state_dict(), os.path.join(FOLDER, \"discrete-policy.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mekor\\Documents\\Skoltech\\Term-7\\RL\\crash-course-to-reinforce\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\mekor\\Documents\\Skoltech\\Term-7\\RL\\crash-course-to-reinforce\\results\\reinforce folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "Validation: 100%|██████████| 10/10 [01:31<00:00,  9.18s/it]\n"
     ]
    }
   ],
   "source": [
    "env = make_env(ENV_NAME, config=CONFIG)\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=FOLDER,\n",
    "    episode_trigger=lambda x: x == 0 or x == (ITERATIONS - 2),\n",
    "    name_prefix=\"discrete-agent\",\n",
    "    video_length=MAX_LENGTH,\n",
    ")\n",
    "model = PolicyNetworkDiscrete(input_dim, HIDDEN_DIM, output_dim)\n",
    "model.load_state_dict(\n",
    "    torch.load(os.path.join(FOLDER, \"discrete-policy.pt\"), weights_only=False)\n",
    ")\n",
    "results = validate(model, env, n_episodes=ITERATIONS, device=device)\n",
    "mp4_to_gif(FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img width=\"600\" src=\"results\\reinforce\\discrete-agent-episode-8.gif\" alt=\"Discrete action space policy\">\n",
    "    <p align=\"center\">Fig. 1 - Policy in 5Hz environment with discrete action space</p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': deque([np.float64(51.0564906666667),\n",
       "        np.float64(114.50666666666639),\n",
       "        np.float64(75.54773333333327),\n",
       "        np.float64(104.50093511111118),\n",
       "        np.float64(122.64000000000009),\n",
       "        np.float64(58.481066666666706),\n",
       "        np.float64(70.21439999999993),\n",
       "        np.float64(120.08888888888906),\n",
       "        np.float64(12.133333333333335),\n",
       "        np.float64(20.417777777777783)],\n",
       "       maxlen=10),\n",
       " 'length': deque([63, 144, 88, 128, 150, 72, 84, 143, 16, 25], maxlen=10),\n",
       " 'norm_length': [0.315,\n",
       "  0.72,\n",
       "  0.44,\n",
       "  0.64,\n",
       "  0.75,\n",
       "  0.36,\n",
       "  0.42,\n",
       "  0.715,\n",
       "  0.08,\n",
       "  0.125]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 10/10 [01:47<00:00, 10.71s/it]\n"
     ]
    }
   ],
   "source": [
    "envs = make_envs(ENV_NAME, THREADS, config=CONFIG)\n",
    "results = validate(model, env, n_episodes=ITERATIONS, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation for the continious environment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG[\"action\"] = {\"type\": \"ContinuousAction\"}\n",
    "envs = make_envs(ENV_NAME, THREADS, config=CONFIG)\n",
    "output_dim = envs.action_space._shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration  1/10: 100%|██████████| 199/199 [00:26<00:00,  7.52it/s]\n",
      "Iteration  2/10:  76%|███████▌  | 151/199 [00:23<00:07,  6.09it/s, length=199, reward=10.7]"
     ]
    }
   ],
   "source": [
    "policy = PolicyNetworkContinuous(input_dim, HIDDEN_DIM, output_dim)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model, results = train(policy, envs, optimizer, GAMMA, ITERATIONS, device, SEED)\n",
    "torch.save(model.state_dict(), os.path.join(FOLDER, \"continious-policy.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
