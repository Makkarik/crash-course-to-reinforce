{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel REINFORCE\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall the REINFORCE algorithm:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t=0}^T R_t \\nabla_\\theta \\log \\pi_\\theta(A_t|S_t) \\right],\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$R_t = \\sum_{t=0}^{T-1} \\gamma^{t'-t}r\\left(S_{t'}, A_{t'} \\right)$$\n",
    "\n",
    "The estimation of the gradient can be approximated with Monte-Carlo methods:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T R_t^{(i)} \\nabla_\\theta \\log \\pi_\\theta(A_t^{(i)}|S_t^{(i)}).\n",
    "$$\n",
    "\n",
    "It is obvious that the sum over $t$ can not be computed in parallel, as the current state of the environment depends on the previous one. However, the sum over $i$ can be computed in parallel, as the trajectories are independent. Thus, we need to launch $N$ environments in parallel, collect the trajectories, and then compute the gradients.\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\underbrace{\\frac{1}{N} \\sum_{i=1}^N}_{\\text{independent}} \\underbrace{\\sum_{t=0}^T R_t^{(i)} \\nabla_\\theta \\log \\pi_\\theta(A_t^{(i)}|S_t^{(i)})}_{\\text{sequential}}.\n",
    "$$\n",
    "\n",
    "Unfortunately the parallelization can not be perfromed straightforwardly, as independent environments reach the terminal states at different times. Moreover, after reaching the terminal state, the environment is automatically restarted, beggining the new run ahead of other environments.\n",
    "\n",
    "$$ S_0^{(i-1)} \\rightarrow S_1^{(i-1)} \\rightarrow ... \\rightarrow S_t^{(i-1)} \\rightarrow S_{t+1}^{(i-1)} \\rightarrow ... $$\n",
    "\n",
    "$$ S_0^{(i)} \\rightarrow S_1^{(i)} \\rightarrow ... \\rightarrow \\underbrace{S_t^{(i)}}_{\\text{terminal}} \\rightarrow \\underbrace{S_0^{(k)}}_{\\text{invalid}}\\rightarrow ... $$\n",
    "\n",
    "$$ S_0^{(i+1)} \\rightarrow S_1^{(i+1)} \\rightarrow ... \\rightarrow S_t^{(i+1)} \\rightarrow S_{t+1}^{(i+1)} \\rightarrow ... $$\n",
    "\n",
    "where $k$ - arbitrary index, indicating the start of the invalid states sequence.\n",
    "\n",
    "Thus, we need to synchronize the environments at the end of each episode. There are two possible ways to do that: truncation \n",
    "\n",
    "$$ S_0^{(i-1)} \\rightarrow S_1^{(i-1)} \\rightarrow ... \\rightarrow \\underbrace{S_t^{(i-1)}}_{\\text{aborted}} $$\n",
    "\n",
    "$$ S_0^{(i)} \\rightarrow S_1^{(i)} \\rightarrow ... \\rightarrow \\underbrace{S_t^{(i)}}_{\\text{terminal}} $$\n",
    "\n",
    "$$ S_0^{(i+1)} \\rightarrow S_1^{(i+1)} \\rightarrow ... \\rightarrow \\underbrace{S_t^{(i+1)}}_{\\text{aborted}} $$\n",
    "\n",
    "and masking\n",
    "\n",
    "$$ S_0^{(i-1)} \\rightarrow S_1^{(i-1)} \\rightarrow ... \\rightarrow S_t^{(i-1)} \\rightarrow S_{t+1}^{(i-1)} \\rightarrow S_{t+2}^{(i-1)} \\rightarrow ... $$\n",
    "\n",
    "$$ S_0^{(i)} \\rightarrow S_1^{(i)} \\rightarrow ... \\rightarrow \\underbrace{S_t^{(i)}}_{\\text{terminal}} \\rightarrow \\underbrace{S_0^{(k)} \\rightarrow S_1^{(k)} }_{\\text{not accounted}} \\rightarrow ... $$\n",
    "\n",
    "$$ S_0^{(i+1)} \\rightarrow S_1^{(i+1)} \\rightarrow ... \\rightarrow S_t^{(i+1)} \\rightarrow S_{t+1}^{(i+1)} \\rightarrow S_{t+2}^{(i+1)} \\rightarrow ... $$\n",
    "\n",
    "\n",
    "In case of truncation, we terminates the current episode when the first environment reaches the terminal state. This approach is not preferable as it prematurely ends the environments that have not reached the terminal state yet, thus limiting the ponentially acessable information about the environment. Padding, on the other hand, is more preferable, as it allows to collect the full trajectories from all the environments. The only thing we need to do is to mask out the logarithm gradients of the states, that took place after the automatic restart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical rationale\n",
    "\n",
    "Recall the nabla distributive function property\n",
    "\n",
    "$$ \\nabla_\\theta \\left( \\alpha f(\\theta) \\right) = \\alpha \\nabla_\\theta f(\\theta), $$\n",
    "\n",
    "therefore \n",
    "\n",
    "$$ \n",
    "\\nabla_\\theta \\left(0 \\cdot f(\\theta) \\right) = 0 \\cdot \\nabla_\\theta f(\\theta) = 0, \\\\\n",
    "\\nabla_\\theta \\left(1 \\cdot f(\\theta) \\right) = 1 \\cdot \\nabla_\\theta f(\\theta) = \\nabla_\\theta f(\\theta).\n",
    "$$\n",
    "\n",
    "Hence, by introducing the masking function $H\\left(S_t^{(i)}\\right)$ we can nullify the gradients of the environments that have reached the terminal state without stopping other environments. The masking function is defined as follows:\n",
    "\n",
    "$$\n",
    "H\\left(S_t^{(i)}\\right) = \\begin{cases}\n",
    "1, & \\text{if } S_k^{(i)} \\text{ is not terminal } \\forall k \\in \\{0,...,t-1\\} \\\\\n",
    "0, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Thus, the gradient for the $\\theta$ for the given environment becomes zero as soon as the environment reaches the terminal state. The final gradient is computed as follows:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\gamma^t R_t^{(i)} H\\left(S_t^{(i)}\\right) \\nabla_\\theta \\log \\pi_\\theta\\left(A_t^{(i)}|S_t^{(i)}\\right).\n",
    "$$ \n",
    "\n",
    "that can be represented as following\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta) &\\approx \\frac{1}{N} \\sum_{t=0}^{T^{(1)}} \\gamma^t R_t^{(1)} \\nabla_\\theta \\log \\pi_\\theta\\left(A_t^{(1)}|S_t^{(1)}\\right) + \\sum_{t=T^{(1)}}^{T} 0 \\, + \\\\\n",
    "\n",
    "&\\vdots\\\\\n",
    "\n",
    "&+ \\frac{1}{N} \\sum_{t=0}^{T^{(i)}} \\gamma^t R_t^{(i)} \\nabla_\\theta \\log \\pi_\\theta\\left(A_t^{(i)}|S_t^{(i)}\\right) + \\sum_{t=T^{(i)}}^{T} 0 \\, + \\\\\n",
    "\n",
    "&\\vdots\\\\\n",
    "\n",
    "&+ \\frac{1}{N} \\sum_{t=0}^{T^{(N)}} \\gamma^t R_t^{(N)} \\nabla_\\theta \\log \\pi_\\theta\\left(A_t^{(N)}|S_t^{(N)}\\right) + \\sum_{t=T^{(N)}}^{T} 0, \\\\\n",
    "\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $T = \\max\\{T^{(1)}, ..., T^{(N)}\\}$.\n",
    "\n",
    "The further steps are evident\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta).$$\n",
    "$$\\text{Repeat until convergence}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "It is know that the gymnasium environments yield tuple of five elements for each step:\n",
    "\n",
    "```python\n",
    "obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "```\n",
    "\n",
    "For the vectorized enviornment, the first dimension of each element is equal to the number of the environments, which is the $N$ in the formula above. Asssuming that, the definition of padding function becomes evident\n",
    "\n",
    "```python\n",
    "class Masking:\n",
    "\n",
    "    def __init__(self, size: int):\n",
    "        \"\"\"Initialize appding function\"\"\"\n",
    "        self._size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the padding function\"\"\"\n",
    "        self._is_terminated = np.full(size=self._size, False)\n",
    "\n",
    "    def __call__(self, terminated: np.ndarray, truncated: np.ndarray) -> torch.Tensor, bool:\n",
    "        \"\"\"Yields the padding values\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        terminated : np.ndarray\n",
    "            The array of the terminal states\n",
    "        truncated : np.ndarray\n",
    "            The array of the truncated states\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The padding values (1 if value should be used, 0 otherwise)\n",
    "        bool\n",
    "            The total termination flag. If True, then all the states have reached the\n",
    "            terminal states and the training loop should be stopped.\n",
    "        \"\"\"\n",
    "        # As the function has a lag, we need firstly yield the previous values\n",
    "        output = torch.Tensor(self._is_terminated.astype(int))\n",
    "        # Then we update the values\n",
    "        done = np.logical_or(terminated, truncated)\n",
    "        self._is_terminated = np.logical_or(self._is_terminated, done)\n",
    "        # As termination flag checked after the step, we can yield it, using the updated\n",
    "        # values\n",
    "        total_termination = np.all(self._is_terminated)\n",
    "        return output, total_termination \n",
    "\n",
    "```\n",
    "\n",
    "In training loop the function can be used as following:\n",
    "\n",
    "```python\n",
    "\n",
    "padding = Padding(size=N)\n",
    "policy: torch.nn.Module = Policy()\n",
    "\n",
    "...\n",
    "\n",
    "gamma = 1\n",
    "for i in range(iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    padding.reset()\n",
    "    memory.reset()\n",
    "    obs, _ = envs.reset()\n",
    "    losses = torch.zeros(N)\n",
    "    \n",
    "    for t in range(T):\n",
    "\n",
    "        action, log_probs = policy(obs)\n",
    "        obs, rewards, terminated, truncated, _ = env.step(action.detached().numpy())\n",
    "        padding_values, total_termination = padding(terminated, truncated)\n",
    "\n",
    "        memory.append(log_probs, rewards, padding_values)\n",
    "\n",
    "        ...\n",
    "\n",
    "        if total_termination:\n",
    "            break\n",
    "    \n",
    "    loss = memory.loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "For simplicity, the padding module can be integrated into `memory` function to reset it without a separated call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from src.agent import PolicyNetworkContinuous, PolicyNetworkDiscrete, train, validate\n",
    "from src.envs import make_env, make_envs\n",
    "from src.utils import mp4_to_gif\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation for the discrete environment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "THREADS = 15\n",
    "ITERATIONS = 10\n",
    "GAMMA = 0.95\n",
    "\n",
    "ENV_NAME = \"highway-fast-v0\"\n",
    "\n",
    "FOLDER = \"./results/reinforce\"\n",
    "\n",
    "CONFIG = {\n",
    "    \"observation\": {\n",
    "        # Use an occupancy grid. The grid size and features can be adjusted.\n",
    "        \"type\": \"OccupancyGrid\",  # or \"Kinematics\" / \"TimeToCollision\"\n",
    "        \"grid_size\": [\n",
    "            [-5, 5],\n",
    "            [-5, 5],\n",
    "        ],  # Two dimensions: x from -5 to 5 and y from -5 to 5\n",
    "        \"grid_step\": [2.0, 2.0],  # Specify step for each dimension\n",
    "        \"features\": [\"presence\", \"vx\"],  # presence and relative speed features\n",
    "    },\n",
    "    \"lanes_count\": 4,\n",
    "    \"vehicles_count\": 50,\n",
    "    \"simulation_frequency\": 25,  # adjust as needed\n",
    "    \"policy_frequency\": 5,\n",
    "    \"duration\": 40,  # initial episode duration in seconds\n",
    "    \"action\": {\"type\": \"DiscreteMetaAction\"},  # use the discrete meta-action space\n",
    "    \"offscreen_rendering\": True,\n",
    "    \"collision_reward\": -1,  # The reward received when colliding with a vehicle.\n",
    "    \"reward_speed_range\": [20, 30],\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\",\n",
    "    \"screen_width\": 600,  # [px]\n",
    "    \"screen_height\": 150,  # [px]\n",
    "    \"centering_position\": [0.3, 0.5],\n",
    "    \"scaling\": 5.5,\n",
    "    \"show_trajectories\": False,\n",
    "    \"render_agent\": True,\n",
    "}\n",
    "\n",
    "MAX_LENGTH = CONFIG[\"duration\"] * CONFIG[\"policy_frequency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = make_envs(ENV_NAME, THREADS, config=CONFIG)\n",
    "input_dim = envs.observation_space.shape[1]\n",
    "output_dim = envs.action_space[0].n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 128\n",
    "LEARNING_RATE = 5e-3\n",
    "\n",
    "policy = PolicyNetworkDiscrete(input_dim, HIDDEN_DIM, output_dim)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration  1/10:  36%|███▌      | 71/199 [00:26<00:48,  2.65it/s]\n",
      "Iteration  2/10:  47%|████▋     | 94/199 [00:36<00:40,  2.61it/s, length=40.7, reward=32.7]\n",
      "Iteration  3/10:  97%|█████████▋| 193/199 [01:23<00:02,  2.32it/s, length=38.5, reward=31.6]\n",
      "Iteration  4/10:  81%|████████▏ | 162/199 [01:13<00:16,  2.20it/s, length=50.3, reward=41.2]\n",
      "Iteration  5/10:  61%|██████    | 121/199 [00:53<00:34,  2.25it/s, length=60.2, reward=47.6]\n",
      "Iteration  6/10: 100%|██████████| 199/199 [01:28<00:00,  2.24it/s, length=46.7, reward=36.6]\n",
      "Iteration  7/10: 100%|██████████| 199/199 [01:30<00:00,  2.19it/s, length=106, reward=80.5]\n",
      "Iteration  8/10:  97%|█████████▋| 193/199 [01:28<00:02,  2.18it/s, length=98.4, reward=74.7]\n",
      "Iteration  9/10: 100%|██████████| 199/199 [01:25<00:00,  2.32it/s, length=59.1, reward=45.6]\n",
      "Iteration 10/10: 100%|██████████| 199/199 [01:26<00:00,  2.29it/s, length=106, reward=80]\n"
     ]
    }
   ],
   "source": [
    "model, results = train(policy, envs, optimizer, GAMMA, ITERATIONS, device, SEED)\n",
    "torch.save(model.state_dict(), os.path.join(FOLDER, \"discrete-policy.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 10/10 [03:18<00:00, 19.84s/it]\n"
     ]
    }
   ],
   "source": [
    "env = make_env(ENV_NAME, config=CONFIG)\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=FOLDER,\n",
    "    episode_trigger=lambda x: x == 0 or x == (ITERATIONS - 2),\n",
    "    name_prefix=\"discrete-agent\",\n",
    "    video_length=MAX_LENGTH,\n",
    ")\n",
    "model = PolicyNetworkDiscrete(input_dim, HIDDEN_DIM, output_dim)\n",
    "model.load_state_dict(\n",
    "    torch.load(os.path.join(FOLDER, \"discrete-policy.pt\"), weights_only=False)\n",
    ")\n",
    "results = validate(model, env, ITERATIONS, device=device)\n",
    "mp4_to_gif(FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img width=\"600\" src=\"results\\reinforce\\discrete-agent-episode-0.gif\" alt=\"Discrete action space policy\">\n",
    "    <p align=\"center\">Fig. 1 - Policy in 5Hz environment with discrete action space</p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': deque([np.float64(146.99033871809775),\n",
       "        np.float64(146.99033871809775),\n",
       "        np.float64(138.1014498292087),\n",
       "        np.float64(146.99033871809775),\n",
       "        np.float64(142.5458942736533),\n",
       "        np.float64(138.1014498292087),\n",
       "        np.float64(146.99033871809775),\n",
       "        np.float64(146.99033871809775),\n",
       "        np.float64(146.99033871809775),\n",
       "        np.float64(146.99033871809775)],\n",
       "       maxlen=10),\n",
       " 'length': deque([200, 200, 200, 200, 200, 200, 200, 200, 200, 200],\n",
       "       maxlen=10),\n",
       " 'norm_length': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation for the continious environment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG[\"action\"] = {\"type\": \"ContinuousAction\"}\n",
    "envs = make_envs(ENV_NAME, THREADS, config=CONFIG)\n",
    "output_dim = envs.action_space._shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration  1/10: 100%|██████████| 199/199 [01:25<00:00,  2.32it/s]\n",
      "Iteration  2/10: 100%|██████████| 199/199 [01:33<00:00,  2.12it/s, length=199, reward=11]\n",
      "Iteration  3/10: 100%|██████████| 199/199 [01:33<00:00,  2.12it/s, length=199, reward=10.3]\n",
      "Iteration  4/10: 100%|██████████| 199/199 [01:34<00:00,  2.10it/s, length=199, reward=16.4]\n",
      "Iteration  5/10: 100%|██████████| 199/199 [01:32<00:00,  2.15it/s, length=199, reward=10.5]\n",
      "Iteration  6/10: 100%|██████████| 199/199 [01:27<00:00,  2.27it/s, length=199, reward=12.9]\n",
      "Iteration  7/10: 100%|██████████| 199/199 [01:27<00:00,  2.27it/s, length=199, reward=18.4]\n",
      "Iteration  8/10: 100%|██████████| 199/199 [01:28<00:00,  2.26it/s, length=199, reward=18]\n",
      "Iteration  9/10: 100%|██████████| 199/199 [01:28<00:00,  2.24it/s, length=199, reward=13.8]\n",
      "Iteration 10/10: 100%|██████████| 199/199 [01:36<00:00,  2.05it/s, length=199, reward=15.7]\n"
     ]
    }
   ],
   "source": [
    "policy = PolicyNetworkContinuous(input_dim, HIDDEN_DIM, output_dim)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model, results = train(policy, envs, optimizer, GAMMA, ITERATIONS, device, SEED)\n",
    "torch.save(model.state_dict(), os.path.join(FOLDER, \"continuous-policy.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mekor\\Documents\\Skoltech\\Term-7\\RL\\crash-course-to-reinforce\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\mekor\\Documents\\Skoltech\\Term-7\\RL\\crash-course-to-reinforce\\results\\reinforce folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "Validation: 100%|██████████| 10/10 [03:18<00:00, 19.83s/it]\n"
     ]
    }
   ],
   "source": [
    "env = make_env(ENV_NAME, config=CONFIG)\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=FOLDER,\n",
    "    episode_trigger=lambda x: x == 0 or x == (ITERATIONS - 2),\n",
    "    name_prefix=\"continuous-agent\",\n",
    "    video_length=MAX_LENGTH,\n",
    ")\n",
    "model = PolicyNetworkContinuous(input_dim, HIDDEN_DIM, output_dim)\n",
    "model.load_state_dict(\n",
    "    torch.load(os.path.join(FOLDER, \"continuous-policy.pt\"), weights_only=False)\n",
    ")\n",
    "results = validate(model, env, ITERATIONS, device=device)\n",
    "mp4_to_gif(FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': deque([np.float64(12.771006042721226),\n",
       "        np.float64(12.771006042721226),\n",
       "        np.float64(24.39750661519068),\n",
       "        np.float64(24.39750661519068),\n",
       "        np.float64(24.39750661519068),\n",
       "        np.float64(28.04195105963513),\n",
       "        np.float64(19.093958033192482),\n",
       "        np.float64(12.771006042721226),\n",
       "        np.float64(19.093958033192482),\n",
       "        np.float64(12.771006042721226)],\n",
       "       maxlen=10),\n",
       " 'length': deque([200, 200, 200, 200, 200, 200, 200, 200, 200, 200],\n",
       "       maxlen=10),\n",
       " 'norm_length': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
